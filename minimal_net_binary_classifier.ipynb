{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on minimal_net cs231n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 1.872996\n",
      "iteration 10: loss 0.243096\n",
      "iteration 20: loss 0.131005\n",
      "iteration 30: loss 0.093438\n",
      "iteration 40: loss 0.075212\n",
      "iteration 50: loss 0.064682\n",
      "iteration 60: loss 0.057941\n",
      "iteration 70: loss 0.053323\n",
      "iteration 80: loss 0.050001\n",
      "iteration 90: loss 0.047523\n",
      "iteration 100: loss 0.045621\n",
      "iteration 110: loss 0.044126\n",
      "iteration 120: loss 0.042928\n",
      "iteration 130: loss 0.041952\n",
      "iteration 140: loss 0.041146\n",
      "iteration 150: loss 0.040471\n",
      "iteration 160: loss 0.039899\n",
      "iteration 170: loss 0.039411\n",
      "iteration 180: loss 0.038989\n",
      "iteration 190: loss 0.038621\n",
      "[[-3.23369748  5.00542509]\n",
      " [ 1.40618408  1.22956322]\n",
      " [ 1.91497094 -1.18614417]]\n",
      "[[ 0.41505245 -0.41505245]]\n",
      " for [0,0,1] -  [[0.9807578 0.0192422]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import exp, array, random, dot\n",
    "\n",
    "#in this example we will train linear classifier and compare resultes witb nn\n",
    "\n",
    "D=3\n",
    "C =2 \n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]])\n",
    "#Train a Linear Classifier\n",
    "\n",
    "#Initialize randomly weights vector(size 2)\n",
    "np.random.seed(0)\n",
    "W =np.random.randn(D,C)\n",
    "b = np.zeros((1,C))\n",
    "\n",
    "#some hyperparameters\n",
    "\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = training_set_inputs.shape[0]\n",
    "for i in range(200):\n",
    "    # evaluate class scores, [N x K]\n",
    "    scores = np.dot(training_set_inputs, W) + b\n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # [N x K]\n",
    "\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),training_set_outputs])\n",
    "    data_loss = np.sum(corect_logprobs) / num_examples\n",
    "    reg_loss = 0.5 * reg * np.sum(W * W)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),training_set_outputs] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(training_set_inputs.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "    dW += reg * W  # regularization gradient\n",
    "\n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "\n",
    "print(W)\n",
    "print(b)\n",
    "\n",
    "\n",
    "scores = np.dot([0,0,1], W) + b\n",
    "# compute the class probabilities\n",
    "exp_scores = np.exp(scores)\n",
    "probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # [N x K]\n",
    "print(' for [0,0,1] - ', probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.693008\n",
      "iteration 100: loss 0.011667\n",
      "iteration 200: loss 0.011240\n",
      "iteration 300: loss 0.011153\n",
      "iteration 400: loss 0.011096\n",
      "iteration 500: loss 0.011049\n",
      "iteration 600: loss 0.011008\n",
      "iteration 700: loss 0.010973\n",
      "iteration 800: loss 0.010941\n",
      "iteration 900: loss 0.010914\n",
      "iteration 1000: loss 0.010889\n",
      "iteration 1100: loss 0.010868\n",
      "iteration 1200: loss 0.010848\n",
      "iteration 1300: loss 0.010831\n",
      "iteration 1400: loss 0.010816\n",
      "iteration 1500: loss 0.010803\n",
      "iteration 1600: loss 0.010791\n",
      "iteration 1700: loss 0.010780\n",
      "iteration 1800: loss 0.010771\n",
      "iteration 1900: loss 0.010762\n"
     ]
    }
   ],
   "source": [
    "# nn with 1 hidden layer\n",
    "#initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,C)\n",
    "b2 = np.zeros((1,C))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = training_set_inputs.shape[0]\n",
    "for i in range(2000):\n",
    "  \n",
    "  # evaluate class scores, [N x C]\n",
    "  hidden_layer = np.maximum(0, np.dot(training_set_inputs, W) + b) # note, ReLU activation\n",
    "  scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "  # compute the class probabilities\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x C]\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "  corect_logprobs = -np.log(probs[range(num_examples),training_set_outputs])\n",
    "  data_loss = np.sum(corect_logprobs)/num_examples\n",
    "  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "  loss = data_loss + reg_loss\n",
    "  if i % 100 == 0:\n",
    "    print (\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(num_examples),training_set_outputs] -= 1\n",
    "  dscores /= num_examples\n",
    "  \n",
    "  # backpropate the gradient to the parameters\n",
    "  # first backprop into parameters W2 and b2\n",
    "  dW2 = np.dot(hidden_layer.T, dscores)\n",
    "  db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "  # next backprop into hidden layer\n",
    "  dhidden = np.dot(dscores, W2.T)\n",
    "  # backprop the ReLU non-linearity\n",
    "  dhidden[hidden_layer <= 0] = 0\n",
    "  # finally into W,b\n",
    "  dW = np.dot(training_set_inputs.T, dhidden)\n",
    "  db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "  # add regularization gradient contribution\n",
    "  dW2 += reg * W2\n",
    "  dW += reg * W\n",
    "  \n",
    "  # perform a parameter update\n",
    "  W += -step_size * dW\n",
    "  b += -step_size * db\n",
    "  W2 += -step_size * dW2\n",
    "  b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(training_set_inputs, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print ('training accuracy: %.2f' % (np.mean(predicted_class == training_set_outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for [0,0,1] -  [[0.99843542 0.00156458]]\n"
     ]
    }
   ],
   "source": [
    "scores =np.dot(np.dot([0,0,1], W)+b,W2)+b2\n",
    "# compute the class probabilities\n",
    "exp_scores = np.exp(scores)\n",
    "probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # [N x K]\n",
    "print(' for [0,0,1] - ', probs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
